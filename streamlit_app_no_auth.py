import streamlit as st
import os
import time
import re
from langchain_pinecone import PineconeVectorStore
from langchain_openai import OpenAIEmbeddings
from langchain_groq import ChatGroq
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferWindowMemory
from langchain_core.prompts import ChatPromptTemplate

# Page title and configuration
st.set_page_config(
    page_title="QUIC.cloud & LiteSpeed Assistant",
    page_icon="üöÄ",
    layout="wide"
)

# Custom CSS for button colors and improving loading display
st.markdown("""
<style>
    .stButton button {
        background-color: #FF4B4B;
        color: white;
    }
    .stButton button:hover {
        background-color: #D44141;
        color: white;
    }
    .stButton button:focus {
        background-color: #D44141;
        color: white;
    }
    /* Hide empty streamlit elements */
    .element-container:has(.stSpinner) + div:has(div:empty) {
        display: none;
    }
    /* Improved styling for chat messages */
    .stChatMessage div {
        max-width: 95% !important; 
    }
    /* Better spacing for paragraphs in chat */
    .stChatMessage p {
        margin-bottom: 0.8em !important;
    }
    /* Improve code block readability */
    .stChatMessage code {
        white-space: pre-wrap !important;
    }
</style>
""", unsafe_allow_html=True)

# Set API keys from environment variables or secrets
def set_api_keys():
    # Try to get from environment variables first, then from secrets
    openai_key = os.getenv('OPENAI_API_KEY') or st.secrets.get("api_keys", {}).get("openai", "")
    pinecone_key = os.getenv('PINECONE_API_KEY') or st.secrets.get("api_keys", {}).get("pinecone", "")
    groq_key = os.getenv('GROQ_API_KEY') or st.secrets.get("api_keys", {}).get("groq", "")
    
    if openai_key:
        os.environ['OPENAI_API_KEY'] = openai_key
    if pinecone_key:
        os.environ['PINECONE_API_KEY'] = pinecone_key
    if groq_key:
        os.environ['GROQ_API_KEY'] = groq_key

# Custom prompt template for expert responses
CUSTOM_PROMPT = """
# Role and Identity
You are an expert technical support specialist EXCLUSIVELY for QUIC.cloud and LiteSpeed Technologies.
You have deep knowledge of:
- WordPress optimization and caching
- QUIC.cloud CDN, Image Optimization, and other services
- LiteSpeed Web Server and LiteSpeed Cache plugin
- DNS configuration, domains, and subdomains
- Server administration and PHP configuration
- Website performance optimization

# Information Context
Analyze the following information from the QUIC.cloud and LiteSpeed documentation:

<context>
{context}
</context>

User Question: {question}

# ‚ö†Ô∏è STRICT TOPIC ENFORCEMENT - HIGHEST PRIORITY ‚ö†Ô∏è
- You are FORBIDDEN from answering ANY questions not DIRECTLY related to QUIC.cloud or LiteSpeed documentation.
- For ANY off-topic questions, respond ONLY with: "I'm sorry, but I can only provide information about QUIC.cloud and LiteSpeed products and services. Please ask a question related to these topics."
- FORBIDDEN TOPICS (always refuse these):
  * General Linux/Unix commands or administration not specific to LiteSpeed
  * Generic programming help or coding questions
  * General server management unrelated to LiteSpeed
  * Web development topics not specific to QUIC.cloud/LiteSpeed
  * Competitor products or services
  * Non-technical or personal questions
  * Instructions for tasks unrelated to QUIC.cloud/LiteSpeed
- EXAMPLES of forbidden questions (NEVER answer these):
  * "How do I delete files older than 7 days in Linux?"
  * "Give me a bash script to automate backups"
  * "How do I set up Nginx with WordPress?"
  * "What's the best way to learn JavaScript?"
- NEVER try to relate off-topic questions back to LiteSpeed/QUIC.cloud
- DO NOT provide workarounds or partial answers to off-topic questions
- If uncertain if a question is on-topic, err on the side of refusing to answer

# Response Guidelines (ONLY for on-topic questions)
1. ANSWER FIRST, REFER SECOND:
   - Provide a complete, detailed answer to the user's question first
   - Only after fully answering the question, mention relevant documentation links
   - Never just tell the user to check documentation without first providing a substantive answer
   - Include all necessary details in your answer rather than relying on references

2. PERSONALIZED RESPONSES (HIGHEST PRIORITY):
   - ALWAYS use any personal information the user provides to personalize your response
   - If they mention their domain name (e.g., example.com), USE IT in your examples and solutions
   - If they mention specific server configurations, plugin versions, or other personal details, INCORPORATE these into your answer
   - Show how solutions apply specifically to their situation, not just generic advice
   - Address their exact scenario rather than providing general information
   - Adapt technical explanations to match their apparent level of expertise

3. KNOWLEDGE BOUNDARIES:
   - Answer based ONLY on the information provided in the context
   - If the question is partially covered, address what you can and acknowledge limitations
   - If the question is completely unrelated to QUIC.cloud or LiteSpeed, use the EXACT refusal message specified above
   - Do not make up information if it's not in the context

4. DETAIL AND DEPTH:
   - Provide comprehensive, well-explained responses that demonstrate expertise
   - Include technical details when appropriate for the user's question
   - Use examples to illustrate concepts where helpful
   - Explain technical reasoning behind recommendations
   - Don't oversimplify complex topics - provide sufficient depth for understanding

5. CLARITY AND STRUCTURE:
   - Begin with a direct answer to the primary question
   - Organize longer responses with a logical flow of information
   - For technical procedures, include clear numbered steps
   - Use paragraphs to separate distinct ideas or aspects of the answer
   - End with a summary or conclusion that reinforces key points

6. CITATIONS:
   - Only after providing a complete answer, reference sources using the permanent_link from the context
   - Mention specific documentation sections when relevant
   - Present references as supplementary material, not primary answers

7. TONE AND STYLE:
   - Be professional, helpful, and technically precise
   - Use clear language that matches the user's technical level
   - For formatting, use ONLY bold (**text**), italic (*text*), or numbered lists
   - DO NOT use large headings or excessive formatting
   - Keep your formatting style consistent throughout the response
   - Use plain text for most content with selective bold or italic for emphasis

Remember: Be accurate, detailed, and solution-oriented. Always give a complete answer first, then references second. 
ALWAYS incorporate the user's personal details (domain names, server info, etc.) when provided to make your response specifically relevant to their situation. If you don't know something with certainty based on the provided context, acknowledge limitations rather than inventing information.
FINAL REMINDER: You MUST ONLY answer questions about QUIC.cloud and LiteSpeed products/services based on the provided documentation. For ANY off-topic question, respond ONLY with the exact refusal message. No exceptions.
"""

# Define custom prompt template
prompt = ChatPromptTemplate.from_template(CUSTOM_PROMPT)

# Format response to ensure consistent styling
def format_response(text):
    """Ensure response has consistent formatting without large headings."""
    # Remove any markdown headings (# and ##) but keep the text
    text = re.sub(r'^#\s+(.+)$', r'**\1**', text, flags=re.MULTILINE)
    text = re.sub(r'^##\s+(.+)$', r'**\1**', text, flags=re.MULTILINE)
    # Keep ### as bold
    text = re.sub(r'^###\s+(.+)$', r'**\1**', text, flags=re.MULTILINE)
    # Keep #### and beyond as is, they're small enough
    
    # Ensure consistent spacing
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    return text

# Function to generate response chunks
def generate_response_chunks(result):
    """Generate response chunks from the response dictionary."""
    answer = result['answer']
    # Process the answer to ensure consistent formatting
    answer = format_response(answer)
    words = answer.split(' ')
    
    # Yield words one by one
    current_text = ""
    for word in words:
        current_text += word + " "
        yield word + " "  # Yield just the current word
        time.sleep(0.03)  # Add a slight delay between words for natural typing effect

# Create retrieval chain
def create_retrieval_chain(vectorstore):
    """Create a retrieval chain with a new memory instance to avoid session overlaps"""
    # Get memory from session state to ensure it's session-specific
    memory = st.session_state.memory
    
    # Initialize Groq LLM with moderate max_tokens
    llm = ChatGroq(
        temperature=0.3,
        model_name="meta-llama/llama-4-scout-17b-16e-instruct",
        max_tokens=4096  # Moderate token limit to balance detail and efficiency
    )
    
    # Create the retrieval chain with custom prompt
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(
            search_kwargs={"k": 5}  # Retrieve 5 documents for more comprehensive context
        ),
        memory=memory,
        return_source_documents=True,
        combine_docs_chain_kwargs={"prompt": prompt},
        verbose=False
    )
    
    return qa_chain

# Main application function
def main():
    # Header
    st.title("üöÄ QUIC.cloud & LiteSpeed Assistant")
    st.markdown("""
    This assistant provides detailed answers about QUIC.cloud and LiteSpeed products and services.
    Ask questions about CDN configuration, WordPress optimization, DNS settings, and more!
    """)

    # Initialize session state for conversation history display
    # Limited to most recent 4 exchanges for UI display
    if "messages" not in st.session_state:
        st.session_state.messages = []
        
    # Initialize flag to track if we're processing a new query
    if "processing_query" not in st.session_state:
        st.session_state.processing_query = False
        
    # Initialize current response buffer
    if "current_response" not in st.session_state:
        st.session_state.current_response = None
    
    # Keep UI chat history limited to 4 exchanges (8 messages)
    if len(st.session_state.messages) > 8:
        st.session_state.messages = st.session_state.messages[-8:]
        
    # Ensure memory is initialized with window size of 3
    if "memory" not in st.session_state:
        st.session_state.memory = ConversationBufferWindowMemory(
            return_messages=True,
            memory_key="chat_history",
            output_key="answer",
            k=3  # Only keep the 3 most recent message exchanges
        )

    # Initialize embeddings and vectorstore only once
    @st.cache_resource
    def initialize_vectorstore():
        """Initialize embeddings and vectorstore"""
        try:
            # Initialize embeddings
            embeddings = OpenAIEmbeddings(
                model="text-embedding-3-small"
            )
            
            # Initialize Pinecone vector store
            index_name = "docschatbot"
            vectorstore = PineconeVectorStore.from_existing_index(
                index_name=index_name,
                embedding=embeddings
            )
            
            return vectorstore, True
        except Exception as e:
            st.error(f"Error initializing the vector store: {str(e)}")
            return None, False
    
    # Initialize vector store
    vectorstore, vectorstore_success = initialize_vectorstore()
    
    # Display previous chat messages (limited to most recent)
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if "sources" in message and message["sources"]:
                with st.expander("Additional Reference Material"):
                    st.markdown("_The following resources contain more information on this topic:_")
                    for idx, source in enumerate(message["sources"], 1):
                        st.markdown(f"**{idx}. {source['title']}**")
                        st.markdown(f"[Read more in documentation]({source['permanent_link']})")

    # Get user input
    user_query = st.chat_input("Ask a question about QUIC.cloud or LiteSpeed...")

    # Process user query
    if user_query and vectorstore_success and not st.session_state.processing_query:
        # Set processing flag to prevent duplicate processing
        st.session_state.processing_query = True
        
        # Add user message to chat history UI
        st.session_state.messages.append({"role": "user", "content": user_query})
        
        # Keep UI chat history limited to 4 exchanges (8 messages)
        if len(st.session_state.messages) > 8:
            st.session_state.messages = st.session_state.messages[-8:]
        
        # Display user message in a new container to avoid conflicting with history display
        with st.chat_message("user"):
            st.markdown(user_query)
        
        # Create a dedicated assistant container for the new response
        assistant_placeholder = st.empty()
        with assistant_placeholder.chat_message("assistant"):
            # Create placeholder for content
            message_placeholder = st.empty()
            sources_placeholder = st.empty()
            
            # Create retrieval chain with fresh memory instance
            qa_chain = create_retrieval_chain(vectorstore)
            
            with st.spinner("Searching documentation..."):
                start_time = time.time()
                
                try:
                    # Process the query
                    result = qa_chain.invoke({"question": user_query})
                    
                    # Extract sources
                    source_docs = result.get('source_documents', [])
                    
                    # Format source documents
                    sources = []
                    for doc in source_docs:
                        if doc.metadata:
                            sources.append({
                                'title': doc.metadata.get('title', 'Document'),
                                'url': doc.metadata.get('url', ''),
                                'permanent_link': doc.metadata.get('permanent_link', '')
                            })
                    
                    # Get the complete response first to avoid streaming issues
                    complete_response = format_response(result['answer'])
                    
                    # Use the message placeholder to display the response properly
                    message_placeholder.markdown(complete_response)
                    
                    # Display sources in an expander
                    if sources:
                        with sources_placeholder.expander("Additional Reference Material"):
                            st.markdown("_The following resources contain more information on this topic:_")
                            for idx, source in enumerate(sources, 1):
                                st.markdown(f"**{idx}. {source['title']}**")
                                if source['permanent_link']:
                                    st.markdown(f"[Read more in documentation]({source['permanent_link']})")
                    
                    # Store the response in session state for UI display
                    st.session_state.messages.append({
                        "role": "assistant", 
                        "content": complete_response,
                        "sources": sources
                    })
                    
                    # Keep UI chat history limited to 4 exchanges (8 messages)
                    if len(st.session_state.messages) > 8:
                        st.session_state.messages = st.session_state.messages[-8:]
                    
                    # Show response time
                    st.caption(f"Response time: {time.time() - start_time:.2f} seconds")
                    
                except Exception as e:
                    error_msg = f"Error processing your query: {str(e)}"
                    message_placeholder.error(error_msg)
                    st.session_state.messages.append({"role": "assistant", "content": error_msg})
                    
                    # Keep UI chat history limited to 4 exchanges (8 messages)
                    if len(st.session_state.messages) > 8:
                        st.session_state.messages = st.session_state.messages[-8:]
                
                # Reset processing flag when done
                st.session_state.processing_query = False
                st.session_state.current_response = None

    # Sidebar with information
    with st.sidebar:
        st.title("About this Assistant")
        st.markdown("""
        ### QUIC.cloud & LiteSpeed Assistant
        
        This AI assistant provides comprehensive answers about:
        - QUIC.cloud services and features
        - LiteSpeed Web Server configuration
        - LiteSpeed Cache for WordPress
        - CDN setup and optimization
        - Domain and DNS management
        - Caching strategies and best practices
        - Performance optimization techniques
        - WordPress plugin configuration
        
        ### Need More Help?
        
        If you can't find what you need, open a support ticket:
        [QUIC.cloud Support](https://account.quic.cloud/plugin/support_manager/client_tickets/departments/)
        
        Or email: support@quic.cloud
        """)
        
        # Clear conversation button
        if st.button("Clear Conversation"):
            st.session_state.messages = []
            # Reset memory on conversation clear
            st.session_state.memory = ConversationBufferWindowMemory(
                return_messages=True,
                memory_key="chat_history",
                output_key="answer",
                k=3  # Only keep the 3 most recent message exchanges
            )
            # Reset processing flag
            st.session_state.processing_query = False
            st.session_state.current_response = None
            st.rerun()

# Main execution
if __name__ == "__main__":
    # Set API keys from environment variables or secrets
    set_api_keys()
    
    # Run the main app directly without authentication
    main() 